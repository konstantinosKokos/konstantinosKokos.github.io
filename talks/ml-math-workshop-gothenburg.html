<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en">
  <head>
    <title>Konstantinos Kogkalidis - LLM-free Representation Learning from Theorem Structure </title>
    <meta charset="UTF-8">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="../css/all.css" />
  </head>

  <body>
    <div id="page">
      <div id="header">
          <ul class="nav-list">
  <li class="nav-elem nav-left nav-head">
    <a href="../">Home</a>
  </li>
  <li class="nav-elem nav-left ">
    <a href="../publications.html">Publications</a>
  </li>
  <li class="nav-elem nav-left ">
    <a href="../talks.html">Talks</a>
  </li>
  <li class="nav-elem nav-left">
    <a href="../cv.html">CV</a>
  </li>
  <li class="nav-elem nav-left">
    <a href="https://github.com/konstantinosKokos" target="_blank">Software</a>
  </li>
</ul>

      </div>
      <div id="content">
            <h2>LLM-free Representation Learning from Theorem Structure</h2>
<h3>
  
    <div>Invited @ <a href="https://www.chalmers.se/en/current/calendar/deep-learning-models-for-mathematics-and-type-theory/"><b>Deep-Learning Models for Mathematics and Type Theory</b></a></div>
  
  April 21, 2025
  
</h3>
<hr>
<p>
<p>Sequential autoregressive models have become a popular backend for automated theorem proving due to their compatibility with pretrained language models. However, this approach reduces the inherently structured nature of theorems and proofs to sequential text, misrepresenting their syntax and semantics. In this work, we introduce a structural alternative tailored for dependent type theory. Our contributions include the first dataset of Agda program-proofs, extracted to capture fine-grained type-level details rather than surface-level representations. Using this dataset, we propose QUILL, a neural architecture designed to model the structural and relational aspects of dependently-typed programs. Despite its small size and limited training data, QUILL achieves strong results, surpassing traditional Transformer-style models by up to an order of magnitude on standard performance metrics.</p>
</p>

      </div>
  </div>
  </body>
  <script type="text/javascript" src="js/collapsible.js"></script>
</html>
